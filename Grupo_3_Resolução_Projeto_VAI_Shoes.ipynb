{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto - Case VAI Shoes\n",
        "##0 - Instalando e importando bibliotecas\n",
        "Nesta etapa instalamos e importamos as principais bibliotecas utilizadas na construção do notebook."
      ],
      "metadata": {
        "id": "2bT6sFjlivDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgBWxti1h1BD"
      },
      "outputs": [],
      "source": [
        "# Import das libs utilizadas\n",
        "\n",
        "# Manipulação de dados\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from textwrap import wrap\n",
        "# Gráficos\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "# Modelagem - Clusterização\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sqlalchemy import create_engine\n",
        "# Modelagem - Regressão\n",
        "# Modelagem - Série Temporal\n",
        "from keras.models import Sequential #!pip install keras e !pip install tensorflow\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from sklearn.preprocessing import StandardScaler #!pip install sklearn\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#Importando fbprophet\n",
        "from prophet import Prophet #maneira de instalação e importação mudou recentemente\n",
        "# Modelagem - Análise de cesta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo função que fornece a quantidade e os valores único de cada feature de um dataframe\n",
        "def get_values_unique(df):\n",
        "    \"\"\"\"\n",
        "    Função que, fornecido um DataFrame, printa a quantidade de valores\n",
        "    únicos e os valores únicos de cada feature.\n",
        "\n",
        "    In: DataFrame.\n",
        "\n",
        "    Out: Print da quantidade e valores únicos da feature.\n",
        "    \"\"\"\n",
        "    for column in df.columns:\n",
        "        print(f'{column} possui {df[column].nunique()} valores únicos:')\n",
        "        print(f'\\n{df[column].unique()}')\n",
        "        print('\\n*********************************************\\n')"
      ],
      "metadata": {
        "id": "cQa8c3PaitBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Aquisição dos dados"
      ],
      "metadata": {
        "id": "jKGKEOIEi_-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Carregamento das bases de dados"
      ],
      "metadata": {
        "id": "mJKWM1XtjCvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lê arquivos ----------------------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Z7JVI0b3jIRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Visualização dos DataFrames e suas informações"
      ],
      "metadata": {
        "id": "i1nIxu0tjMqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Visualização dos valores únicos dos DataFrames"
      ],
      "metadata": {
        "id": "wnLkhkB2jUYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando valores únicos das features de df_vendas\n",
        "get_values_unique(df_vendas)"
      ],
      "metadata": {
        "id": "l3O3aAFTjVAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando valores únicos das features de df_previs\n",
        "get_values_unique(df_previs)"
      ],
      "metadata": {
        "id": "nLvPjteDjWqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pontos sobre a análise inicial dos dados."
      ],
      "metadata": {
        "id": "QL6P17wRjbAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tendências no mercado calçadista:\n",
        "\n",
        "Sustentabilidade: Há uma crescente demanda por calçados produzidos de forma sustentável, com materiais recicláveis ou de origem responsável. Os consumidores estão mais conscientes sobre as questões ambientais e buscam opções ecologicamente corretas.\n",
        "Conforto e funcionalidade: Os consumidores têm dado preferência a calçados que oferecem conforto, suporte e durabilidade. Calçados com tecnologias inovadoras, como solados ergonômicos e materiais respiráveis, têm ganhado destaque.\n",
        "Customização: A personalização de calçados tem se tornado uma tendência relevante, permitindo que os consumidores criem produtos únicos de acordo com suas preferências e estilo."
      ],
      "metadata": {
        "id": "2gI_lQOoUkTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fatores de impacto:\n",
        "\n",
        "Economia: O desempenho econômico do país tem um impacto significativo no mercado calçadista. Em momentos de recessão, por exemplo, as vendas podem diminuir devido à redução do poder de compra dos consumidores.\n",
        "Moda e estilo: Tendências de moda e estilo influenciam a demanda por calçados. A indústria deve estar atenta às preferências e mudanças de comportamento dos consumidores para se adaptar e oferecer produtos atualizados.\n",
        "Custo dos materiais: O preço e a disponibilidade de matérias-primas, como couro, borracha e tecidos, afetam diretamente os custos de produção e, consequentemente, os preços dos calçados.\n",
        "\n",
        "Concorrência internacional: A competição com produtos importados, especialmente da Ásia, é um desafio para a indústria calçadista brasileira. As diferenças de custo de produção e acordos comerciais podem impactar a competitividade dos calçados nacionais."
      ],
      "metadata": {
        "id": "kYhkNbgZUoL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_\"O ano de 2020 foi marcado pela pandemia, resultando em uma queda de 18,4% na produção\n",
        "brasileira de calçados. A indústria calçadista foi impactada por meio de diversos vetores. Um\n",
        "levantamento realizado pela Abicalçados em fevereiro de 2021 sobre a percepção das empresas\n",
        "quanto aos impactos da Covid-19 nos seus negócios destacou que, em 2020, o fechamento do\n",
        "comércio, a queda do faturamento e a redução da demanda foram os principais vetores de impactos da pandemia sobre a indústria calçadista.\"_"
      ],
      "metadata": {
        "id": "L_B8NJrHUqwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Informações Adicionais:_\n",
        "\n",
        "* _Brasil está no TOP 5 produtores mundiais_\n",
        "* _Mulheres dominam o mercado calçadista (70% dele)_"
      ],
      "metadata": {
        "id": "o3gBu16jUs9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Análise Exploratório dos Dados"
      ],
      "metadata": {
        "id": "AFu39KtrVil8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Qual é o total de vendas por mês?\n",
        "vendas_mensais = pd.read_csv(r'/content/drive/MyDrive/VAI_SHOES/dados/tbl_vendas_mensais.csv', sep=';')"
      ],
      "metadata": {
        "id": "GHt2XvV8VwQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_A base de dados vai de Jan/2019 a Mar/2023_"
      ],
      "metadata": {
        "id": "Z-duBPsJV0Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_mensais['vl_sale_price'] = vendas_mensais['vl_sale_price'].apply(lambda x: float(str(x).replace(',', '.')))"
      ],
      "metadata": {
        "id": "d9houNV2V2Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Houve um possivel erro ao carregar os dados de Novembro de 2021, os dados estão duplicados mas com valores de vendas diferentes, para contornar esse problema, vamos utilizar a média entre os dois valores_"
      ],
      "metadata": {
        "id": "iEQJgGcEV2pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_mensais = vendas_mensais.groupby(['dt_sale','id_sku','ds_sku','ds_category','ds_product_line','ds_brand_segment','ds_tecnology']).mean().reset_index()\n",
        "sop = sop.groupby(['dt_sale','ds_product_line']).mean().reset_index()"
      ],
      "metadata": {
        "id": "FJMoXQEFV7bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Algumas datas possuem formatação diferente, então para realizar seu tratamento, primeiro vamos transforma-las em data e depois mudar o formato com que elas são exibidas. Como os dados são mensais, vamos deixar a formatação com o primeiro dia do mês, o mês correspondente e o ano_"
      ],
      "metadata": {
        "id": "8DYs61coV9r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_mensais.head(2)"
      ],
      "metadata": {
        "id": "NaSoYptWV_hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Procurando por valores nulos, encontramos que a coluna \"vl_sale_price\" possui 24 valores nulos, para tentar justificar a presença desses valores vamos investigar para avaliar o tipo de tratamento que será utilizado"
      ],
      "metadata": {
        "id": "pFuWqe_VWBRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coluna | Valores nulos\n",
        ":---: | :---:\n",
        "vl_sale_price | 24"
      ],
      "metadata": {
        "id": "3lFI3JhfWJOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Encontramos que os dados vazios correspondem a 2 produtos: NYC Wave e NYC Sub. Que pertencem a mesma categoria, linha, marca e usam a mesma tecnologia. Possivelmente são 2 produtos que foram adicionados recentemente a base de dados da empresa (Abril de 2022)_"
      ],
      "metadata": {
        "id": "IfRt9Nc2WGUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Encontramos que os dados vazios correspondem a 2 produtos: NYC Wave e NYC Sub. Que pertencem a mesma categoria, linha, marca e usam a mesma tecnologia. Possivelmente são 2 produtos que foram adicionados recentemente a base de dados da empresa (Abril de 2022)_"
      ],
      "metadata": {
        "id": "9Mb0MjIyWLrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_mensais[vendas_mensais.vl_sale_price.isnull()][['id_sku', 'ds_sku', 'ds_category', 'ds_product_line',\n",
        "       'ds_brand_segment', 'ds_tecnology']].drop_duplicates()"
      ],
      "metadata": {
        "id": "a7fmtbOrWPkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gera_vl_vazios = vendas_mensais.\\\n",
        "  query('ds_category==\"NYCs Shoes\" and ds_product_line==\"Shoes\" and ds_brand_segment==\"Core\" and ds_tecnology==\"LiteWeave\"').\\\n",
        "  dropna()\n",
        "\n",
        "gera_vl_vazios['unit_price'] = gera_vl_vazios['vl_sale_price']/gera_vl_vazios['qt_sale']"
      ],
      "metadata": {
        "id": "Czj-_wANWQ8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.line(gera_vl_vazios, x='dt_sale', y='unit_price')"
      ],
      "metadata": {
        "id": "o84hh5fTWR8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Como o preço desse conjunto permanece constante no periodo em que esses 2 produtos foram inclusos na base, vamos selecionar os dados que correspondem a média do preço dos produtos da mesma classe e utiliza-los para preencher os valores vazios_"
      ],
      "metadata": {
        "id": "qez_uv_iWU4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preco_novos_calcados = gera_vl_vazios.query(\"dt_sale >= '2022-04-01'\").unit_price.mean()"
      ],
      "metadata": {
        "id": "MVfM-clQWXny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vendas_mensais['vl_sale_price'] = vendas_mensais.apply(lambda x: x['qt_sale'] * preco_novos_calcados if pd.isna(x['vl_sale_price']) else x['vl_sale_price'], axis=1)"
      ],
      "metadata": {
        "id": "KpmvHKqYWZH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Outro problema observado na base de dados é a questão das classes **SNEARKER**  e **SNEAKERS**, que representam o mesmo grupo. Então para corrigir esse problema vamos uni-los_"
      ],
      "metadata": {
        "id": "NAHmDBkIWa2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(vendas_mensais.groupby(vendas_mensais['dt_sale'].dt.year).agg({'qt_sale':'sum'}).reset_index(),\\\n",
        "              x='dt_sale', y='qt_sale')\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Período\", yaxis_title=\"QTD. Vendida\", title='Vendas Anuais')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "qjxUD1IDWcaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(vendas_mensais.groupby(vendas_mensais['dt_sale'].dt.year).agg({'vl_sale_price':'sum'}).reset_index(),\\\n",
        "              x='dt_sale', y='vl_sale_price')\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Período\",  title='Faturamento Anual', yaxis_title='Vendas')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "sBO49ZQzWrql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analisando a série temporal como um todo\n",
        "fig = px.line(vendas_mensais.groupby('dt_sale').agg({'qt_sale':'sum'}).reset_index(),\\\n",
        "              x='dt_sale', y='qt_sale')\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Período\", yaxis_title=\"QTD. Vendida\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZqmzMSaDWvEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agora analisando as series de cada uma das classes de produtos\n",
        "fig = px.line(vendas_mensais.groupby([vendas_mensais.dt_sale, 'ds_product_line']).agg({'qt_sale':'sum'}).reset_index(),\\\n",
        "              x='dt_sale', y='qt_sale', color='ds_product_line')\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Período\", yaxis_title=\"QTD. Vendida\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "C6--voJmWzFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.line(vendas_mensais.groupby([vendas_mensais.dt_sale, 'ds_sku']).agg({'unit_price':'mean'}).reset_index(),\\\n",
        "              x='dt_sale', y='unit_price', color='ds_sku')\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Período\", yaxis_title=\"Preço\", title='Preço médio por produto')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "F00h5bbKW9Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(vendas_mensais.groupby(['ds_sku']).agg({'qt_sale':'sum'}).reset_index().sort_values('qt_sale', ascending=False),\\\n",
        "              x='ds_sku', y='qt_sale', color='ds_sku')\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Período\", yaxis_title=\"Quantidade\", title='Quantidade de produtos vendidos')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "A4v5A2ewXA85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Observa-se que os produtos mais vendidos são: Machina Metal, Max e Nautical Classic\n",
        "fig = px.bar(vendas_mensais.groupby(['ds_sku']).agg({'vl_sale_price':'sum'}).reset_index().sort_values('vl_sale_price', ascending=False),\\\n",
        "              x='ds_sku', y='vl_sale_price', color='ds_sku')\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Período\", yaxis_title=\"Vendas\", title='Vendas Geradas')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "sLAjf_QSXCnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Quando olhamos as vendas o produto que gera maior capital é o Machina (TOP 1) seguido do Nautical Classic (TOP 3) por ultimo o Classic (TOP 5)_\n",
        "\n",
        "_O preço unitário médio de cada classe apresenta flutu_\n",
        "\n",
        "_Agora vamos realizar uma análise estatística dos dados_"
      ],
      "metadata": {
        "id": "Iz8iY81IXPpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = vendas_mensais.groupby(['dt_sale']).agg({'qt_sale':'sum'}).reset_index().qt_sale\n",
        "\n",
        "# Calcule os valores de ACF e PACF\n",
        "lags = 12\n",
        "acf_values = acf(data, nlags=lags)\n",
        "pacf_values = pacf(data, nlags=lags)\n",
        "\n",
        "# Crie os gráficos ACF e PACF como scatter plots com linhas individuais\n",
        "fig_acf = go.Figure(data=[\n",
        "    go.Scatter(x=np.arange(len(acf_values)), y=acf_values, mode='markers', name='ACF'),\n",
        "])\n",
        "\n",
        "for i in range(len(acf_values)):\n",
        "    fig_acf.add_shape(type='line',\n",
        "                      x0=i, y0=acf_values[i],\n",
        "                      x1=i, y1=0,\n",
        "                      line=dict(color='black', width=1))\n",
        "fig_acf.add_shape(\n",
        "    type='line',\n",
        "    x0=0, y0=-0.5,\n",
        "    x1=len(acf_values), y1=-0.5,\n",
        "    line=dict(color='red', width=1, dash='dash')\n",
        ")\n",
        "\n",
        "fig_acf.add_shape(\n",
        "    type='line',\n",
        "    x0=0, y0=0.5,\n",
        "    x1=len(acf_values), y1=0.5,\n",
        "    line=dict(color='red', width=1, dash='dash'))\n",
        "\n",
        "fig_acf.update_layout(\n",
        "    title=\"Autocorrelation Function (ACF)\",\n",
        "    xaxis_title=\"Lag\",\n",
        "    yaxis_title=\"ACF Value\"\n",
        ")\n",
        "\n",
        "fig_pacf = go.Figure(data=[\n",
        "    go.Scatter(x=np.arange(len(pacf_values)), y=pacf_values, mode='markers', name='PACF'),\n",
        "])\n",
        "\n",
        "for i in range(len(pacf_values)):\n",
        "    fig_pacf.add_shape(type='line',\n",
        "                       x0=i, y0=pacf_values[i],\n",
        "                       x1=i, y1=0,\n",
        "                       line=dict(color='black', width=1))\n",
        "fig_pacf.add_shape(\n",
        "    type='line',\n",
        "    x0=0, y0=-0.25,\n",
        "    x1=len(acf_values), y1=-0.25,\n",
        "    line=dict(color='red', width=1, dash='dash')\n",
        ")\n",
        "\n",
        "fig_pacf.add_shape(\n",
        "    type='line',\n",
        "    x0=0, y0=0.25,\n",
        "    x1=len(acf_values), y1=0.25,\n",
        "    line=dict(color='red', width=1, dash='dash'))\n",
        "\n",
        "fig_pacf.update_layout(\n",
        "    title=\"Partial Autocorrelation Function (PACF)\",\n",
        "    xaxis_title=\"Lag\",\n",
        "    yaxis_title=\"PACF Value\"\n",
        ")\n",
        "\n",
        "# Exiba os gráficos\n",
        "fig_acf.show()\n",
        "fig_pacf.show()\n"
      ],
      "metadata": {
        "id": "_fLp_EFSXetL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Pelo grafico ACF podemos observar que nosso problema se trata de uma serie temporal não estacionária que possui autocorrelação. Além disso, ele reforça a ideia da sazonalidade_\n",
        "\n",
        "_Como os spikes do gráfico PACF (Partial Autocorrelate Function) ultrapassam o intervalo demarcado, isso indica que a série possue autocorrelação e é não estacionária. Outro indicativo de que a série é autocorrelacionada é a presença de uma queda suave no gráfico ACF(Autocorrelate Function)_"
      ],
      "metadata": {
        "id": "CySRzDLoXjHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = vendas_mensais.groupby(['dt_sale']).agg({'qt_sale':'sum'}).reset_index()\n",
        "data = data.set_index('dt_sale')\n",
        "decomposition = seasonal_decompose(data['qt_sale'], model='additive', period=12)\n",
        "\n",
        "fig = make_subplots(rows=4, cols=1, shared_xaxes=True)\n",
        "\n",
        "# Criação dos gráficos separados para cada componente\n",
        "fig = make_subplots(rows=4, cols=1, shared_xaxes=True)\n",
        "\n",
        "# Série temporal\n",
        "fig.add_trace(go.Scatter(x=data.index, y=data['qt_sale'], name='Série Temporal'), row=1, col=1)\n",
        "fig.update_yaxes(title_text='Quantidade Vendida', row=1, col=1)\n",
        "\n",
        "# Sazonalidade\n",
        "fig.add_trace(go.Scatter(x=data.index, y=decomposition.seasonal, name='Sazonalidade'), row=2, col=1)\n",
        "fig.update_yaxes(title_text='Sazonalidade', row=2, col=1)\n",
        "\n",
        "# Tendência\n",
        "fig.add_trace(go.Scatter(x=data.index, y=decomposition.trend, name='Tendência'), row=3, col=1)\n",
        "fig.update_yaxes(title_text='Tendência', row=3, col=1)\n",
        "\n",
        "# Resíduos\n",
        "fig.add_trace(go.Scatter(x=data.index, y=decomposition.resid,mode='markers', marker=dict(color='purple'), name='Resíduos'), row=4, col=1)\n",
        "fig.update_yaxes(title_text='Resíduos', row=4, col=1)\n",
        "\n",
        "fig.update_layout(height=800, title_text='Decomposição da Série Temporal')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "icdWePl5Xm9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Nosso problema se baseia em uma série temporal com tendência positiva e observa-se a presença de uma sazonalidade. Em relação aos residuos, os mesmo apresentam média zero, distribuição proxima a normalidade, ainda precisa avaliar a questão da homocedasticidade_"
      ],
      "metadata": {
        "id": "_gvFeDUZXqAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar a função KDE\n",
        "kde_x = np.linspace(decomposition.resid.min(), decomposition.resid.max(), 100)\n",
        "media = np.nanmean(decomposition.resid)\n",
        "residuos_preenchidos = [valor if np.isfinite(valor) else media for valor in decomposition.resid]\n",
        "kde_y = gaussian_kde(residuos_preenchidos)(kde_x)*30\n",
        "\n",
        "# Criar o histograma com a função KDE\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(x=decomposition.resid, histnorm='density', name='Histograma'))\n",
        "fig.add_trace(go.Scatter(x=kde_x, y=kde_y, mode='lines', name='KDE'))\n",
        "\n",
        "# Atualizar o layout do gráfico\n",
        "fig.update_layout(\n",
        "    title=\"Histograma com KDE\",\n",
        "    xaxis_title=\"Valores\",\n",
        "    yaxis_title=\"Densidade\"\n",
        ")\n",
        "\n",
        "# Exibir o gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ODU7Fa90Xq64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Limpeza e pré-processamento de dados"
      ],
      "metadata": {
        "id": "V2JtaZbHjgGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando os dados\n",
        "df = pd.read_csv('/content/tbl_vendas_mensais.csv', sep=';')\n",
        "\n",
        "# Converte a coluna 'dt_sale' para tipo datetime.\n",
        "# Joga fora as colunas não relevantes\n",
        "# Converte a categoria de Sneaker para Sneakers\n",
        "df['dt_sale'] = pd.to_datetime(df['dt_sale'], utc=True)\n",
        "df.drop(columns=['ds_sku', 'ds_category', 'ds_brand_segment', 'ds_tecnology', 'vl_sale_price'], inplace=True)\n",
        "df.loc[df['ds_product_line'] == 'Sneaker', 'ds_product_line'] = 'Sneakers'\n",
        "\n",
        "# Faz a média nos valores de novembro de 2021\n",
        "df_novembro_2021 = df[df['dt_sale'] == '2021-11']\n",
        "df_novembro_2021 = df_novembro_2021.groupby(['id_sku', 'ds_product_line', 'dt_sale']).mean()\n",
        "df_novembro_2021.reset_index(inplace=True)\n",
        "df_novembro_2021 = df_novembro_2021.set_index('dt_sale')\n",
        "\n",
        "# Remove os valores antigos de novembro de 2021 e adiciona os novos\n",
        "df.drop(index=df[df['dt_sale'] == '2021-11'].index, inplace=True)\n",
        "df.set_index('dt_sale', drop=True, inplace=True)\n",
        "df = pd.concat([df, df_novembro_2021])\n",
        "df.drop(columns='id_sku', inplace=True)\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "# Retorna o dataset final. Para cada produto('ds_product_line') temos a quantidade total de vendas.\n",
        "df = df.groupby(['dt_sale', 'ds_product_line']).sum()\n",
        "df.reset_index(inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "9cHZoOjw8vLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 - Implementação dos Modelos**"
      ],
      "metadata": {
        "id": "6zzUCJPmc1Qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Rede Neural Recorrente (arquitetura LSTM)**\n"
      ],
      "metadata": {
        "id": "Xh3RdD2ddSJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[:, ['dt_sale', 'ds_product_line', 'qt_sale']]\n",
        "\n",
        "tbl = df.groupby(['dt_sale', 'ds_product_line']).sum()\n",
        "tbl = tbl.sort_values(by='dt_sale')\n",
        "tbl = tbl.reset_index()\n",
        "\n",
        "#Separando o Df de cada categoria\n",
        "df_Boot = tbl[tbl['ds_product_line'] == 'Boot']\n",
        "df_Sandals = tbl[tbl['ds_product_line'] == 'Sandals']\n",
        "df_Shoes = tbl[tbl['ds_product_line'] == 'Shoes']\n",
        "df_Sneakers = tbl[tbl['ds_product_line'] == 'Sneakers']"
      ],
      "metadata": {
        "id": "aISOWB-AdcZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Função que separa os atributos previsores para treinamento da rede neural\n",
        "def Sepacao_previsores(df):\n",
        "    df_treino = df.iloc[:47,:]\n",
        "\n",
        "    #Escalonando\n",
        "    df_treino = df_treino['qt_sale'].values\n",
        "    df_treino = df_treino.reshape(-1, 1)\n",
        "    escaler = StandardScaler()\n",
        "    df_treino_escalonada = escaler.fit_transform(df_treino)\n",
        "\n",
        "    previsores = []\n",
        "    qt_real = []\n",
        "\n",
        "    #Para a previsão de um dia, nesse caso, são necessários analisar os 12 meses anteriores\n",
        "    for i in range(12, len(df_treino)):\n",
        "        previsores.append(df_treino_escalonada[i - 12:i, 0])\n",
        "        qt_real.append(df_treino_escalonada[i, 0])\n",
        "\n",
        "    #Reformatação para a forma requerida pelo KERAS\n",
        "    previsores, qt_real = np.array(previsores), np.array(qt_real)\n",
        "    previsores = np.reshape(previsores, (previsores.shape[0], previsores.shape[1], 1))\n",
        "    return previsores, qt_real, escaler"
      ],
      "metadata": {
        "id": "KrTeBYc6dcVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Função que aplica a rede neural\n",
        "def RNN(previsores, qt_real):\n",
        "    #Arquitetura da rede neural\n",
        "    regressor = Sequential()\n",
        "    regressor.add(LSTM(units=100, return_sequences=True, input_shape=(previsores.shape[1], 1)))\n",
        "    regressor.add(Dropout(0.3)) #Irá zerar 30% das entradas (Ajuda a previnir overfitting)\n",
        "\n",
        "    regressor.add(LSTM(units=40, return_sequences=True))\n",
        "    regressor.add(Dropout(0.3))\n",
        "\n",
        "    regressor.add(LSTM(units=50, return_sequences=True))\n",
        "    regressor.add(Dropout(0.3))\n",
        "\n",
        "    regressor.add(LSTM(units=60, return_sequences=False))\n",
        "    regressor.add(Dropout(0.2))\n",
        "\n",
        "    #Camada de saída\n",
        "    regressor.add(Dense(units= 1, activation='linear'))\n",
        "\n",
        "    regressor.compile(optimizer='rmsprop', loss='mean_squared_error', metrics = ['mean_absolute_error'])\n",
        "\n",
        "    #mc = ModelCheckpoint('Best_weights.h5', save_best_only=True, monitor='loss', verbose=1) #Salva os melhores pesos da RN\n",
        "\n",
        "    regressor.fit(previsores, qt_real, epochs=200, batch_size=12)\n",
        "    return regressor"
      ],
      "metadata": {
        "id": "MIk2sX0YdcSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#função que a partir do modelo treinado cria por meio de condicionais as colunas m1, m2 e m3\n",
        "def Previsões(df, regressor, escaler):\n",
        "    m1 = []\n",
        "    m2 = []\n",
        "    m3 = []\n",
        "    #Ciclo que vai de setembro até fevereiro\n",
        "    for i in range(45, 51):\n",
        "        #Previsão de Setembro\n",
        "        if i == 45:\n",
        "            df_teste = df.iloc[i,:]\n",
        "            df_B = df['qt_sale']\n",
        "            entradas = df_B[i - 12:i].values\n",
        "            entradas = entradas.reshape(-1, 1)\n",
        "            entradas = escaler.transform(entradas)\n",
        "\n",
        "            for a in range(0, 3):\n",
        "                X_teste = []\n",
        "                X_teste.append(entradas[0: 12, 0])\n",
        "                X_teste = np.array(X_teste)\n",
        "                X_teste = np.reshape(X_teste, (X_teste.shape[0], X_teste.shape[1], 1))\n",
        "                prev = regressor.predict(X_teste)\n",
        "\n",
        "                if a == 2:\n",
        "                    prev = escaler.inverse_transform(prev)\n",
        "                    m3.append(float(prev))\n",
        "                entradas = np.delete(entradas, 0, axis=0) #Retro alimentação, excluindo o primeiro...\n",
        "                entradas = np.insert(entradas, entradas.shape[0], [prev],axis= 0) #E adicionando a previsão com ultimo\n",
        "        #Previsão de Outubro\n",
        "        if i == 46:\n",
        "            df_teste = df.iloc[i,:]\n",
        "            df_B = df['qt_sale']\n",
        "            entradas = df_B[i - 12:i].values\n",
        "            entradas = entradas.reshape(-1, 1)\n",
        "            entradas = escaler.transform(entradas)\n",
        "\n",
        "            for a in range(0, 3):\n",
        "                X_teste = []\n",
        "                X_teste.append(entradas[0: 12, 0])\n",
        "                X_teste = np.array(X_teste)\n",
        "                X_teste = np.reshape(X_teste, (X_teste.shape[0], X_teste.shape[1], 1))\n",
        "                prev = regressor.predict(X_teste)\n",
        "\n",
        "                if a == 1: #M - 2\n",
        "                   prev_inverse = escaler.inverse_transform(prev)\n",
        "                   m2.append(float(prev_inverse))\n",
        "                if a == 2: #M - 3\n",
        "                    prev = escaler.inverse_transform(prev)\n",
        "                    m3.append(float(prev))\n",
        "                entradas = np.delete(entradas, 0, axis=0)\n",
        "                entradas = np.insert(entradas, entradas.shape[0], [prev],axis= 0)\n",
        "        #Previsão de Novembro\n",
        "        if i == 47:\n",
        "            df_teste = df.iloc[i,:]\n",
        "            df_B = df['qt_sale']\n",
        "            entradas = df_B[i - 12:i].values\n",
        "            entradas = entradas.reshape(-1, 1)\n",
        "            entradas = escaler.transform(entradas)\n",
        "\n",
        "            for a in range(0, 3):\n",
        "                X_teste = []\n",
        "                X_teste.append(entradas[0: 12, 0])\n",
        "                X_teste = np.array(X_teste)\n",
        "                X_teste = np.reshape(X_teste, (X_teste.shape[0], X_teste.shape[1], 1))\n",
        "                prev = regressor.predict(X_teste)\n",
        "\n",
        "                if a == 0: #M - 1\n",
        "                   prev_inverse = escaler.inverse_transform(prev)\n",
        "                   m1.append(float(prev_inverse))\n",
        "                if a == 1: #M - 2\n",
        "                   prev_inverse = escaler.inverse_transform(prev)\n",
        "                   m2.append(float(prev_inverse))\n",
        "                if a == 2: #M - 3\n",
        "                    prev = escaler.inverse_transform(prev)\n",
        "                    m3.append(float(prev))\n",
        "                entradas = np.delete(entradas, 0, axis=0)\n",
        "                entradas = np.insert(entradas, entradas.shape[0], [prev],axis= 0)\n",
        "        #Previsão de Dezembro\n",
        "        if i == 48:\n",
        "            df_teste = df.iloc[i,:]\n",
        "            df_B = df['qt_sale']\n",
        "            entradas = df_B[i - 12:i].values\n",
        "            entradas = entradas.reshape(-1, 1)\n",
        "            entradas = escaler.transform(entradas)\n",
        "\n",
        "            for a in range(0, 3):\n",
        "                X_teste = []\n",
        "                X_teste.append(entradas[0: 12, 0])\n",
        "                X_teste = np.array(X_teste)\n",
        "                X_teste = np.reshape(X_teste, (X_teste.shape[0], X_teste.shape[1], 1))\n",
        "                prev = regressor.predict(X_teste)\n",
        "\n",
        "                if a == 0: #M - 1\n",
        "                   prev_inverse = escaler.inverse_transform(prev)\n",
        "                   m1.append(float(prev_inverse))\n",
        "                if a == 1: #M - 2\n",
        "                   prev_inverse = escaler.inverse_transform(prev)\n",
        "                   m2.append(float(prev_inverse))\n",
        "                if a == 2: #M - 3\n",
        "                    prev = escaler.inverse_transform(prev)\n",
        "                    m3.append(float(prev))\n",
        "                entradas = np.delete(entradas, 0, axis=0)\n",
        "                entradas = np.insert(entradas, entradas.shape[0], [prev],axis= 0)\n",
        "        #Previsão de Janeiro\n",
        "        if i == 49:\n",
        "            df_teste = df.iloc[i,:]\n",
        "            df_B = df['qt_sale']\n",
        "            entradas = df_B[i - 12:i].values\n",
        "            entradas = entradas.reshape(-1, 1)\n",
        "            entradas = escaler.transform(entradas)\n",
        "\n",
        "            for a in range(0, 3):\n",
        "                X_teste = []\n",
        "                X_teste.append(entradas[0: 12, 0])\n",
        "                X_teste = np.array(X_teste)\n",
        "                X_teste = np.reshape(X_teste, (X_teste.shape[0], X_teste.shape[1], 1))\n",
        "                prev = regressor.predict(X_teste)\n",
        "\n",
        "                if a == 0: #M - 1\n",
        "                   prev_inverse = escaler.inverse_transform(prev)\n",
        "                   m1.append(float(prev_inverse))\n",
        "                if a == 1: #M - 2\n",
        "                   prev_inverse = escaler.inverse_transform(prev)\n",
        "                   m2.append(float(prev_inverse))\n",
        "\n",
        "                entradas = np.delete(entradas, 0, axis=0)\n",
        "                entradas = np.insert(entradas, entradas.shape[0], [prev],axis= 0)\n",
        "        #Previsão de Fevereiro\n",
        "        if i == 50:\n",
        "            df_teste = df.iloc[i,:]\n",
        "            df_B = df['qt_sale']\n",
        "            entradas = df_B[i - 12:i].values\n",
        "            entradas = entradas.reshape(-1, 1)\n",
        "            entradas = escaler.transform(entradas)\n",
        "\n",
        "            for a in range(0, 3):\n",
        "                X_teste = []\n",
        "                X_teste.append(entradas[0: 12, 0])\n",
        "                X_teste = np.array(X_teste)\n",
        "                X_teste = np.reshape(X_teste, (X_teste.shape[0], X_teste.shape[1], 1))\n",
        "                prev = regressor.predict(X_teste)\n",
        "\n",
        "                if a == 0: #M - 1\n",
        "                   prev_inverse = escaler.inverse_transform(prev)\n",
        "                   m1.append(float(prev_inverse))\n",
        "\n",
        "                entradas = np.delete(entradas, 0, axis=0)\n",
        "                entradas = np.insert(entradas, entradas.shape[0], [prev],axis= 0)\n",
        "\n",
        "    Product = df.iloc[47:]\n",
        "\n",
        "    Product['Previsão_M1'] = m1\n",
        "    Product['Previsão_M2'] = m2\n",
        "    Product['Previsão_M3'] = m3\n",
        "    return Product"
      ],
      "metadata": {
        "id": "SJEBZiIqdcOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chamado das funções e geração das tabelas de previsão\n",
        "previsores, qt_real, escaler = Sepacao_previsores(df_Boot)\n",
        "Boot = Previsões(df_Boot, RNN(previsores, qt_real), escaler)\n",
        "\n",
        "previsores, qt_real, escaler = Sepacao_previsores(df_Sandals)\n",
        "Sandals = Previsões(df_Sandals, RNN(previsores, qt_real), escaler)\n",
        "\n",
        "previsores, qt_real, escaler = Sepacao_previsores(df_Shoes)\n",
        "Shoes = Previsões(df_Shoes, RNN(previsores, qt_real), escaler)\n",
        "\n",
        "previsores, qt_real, escaler = Sepacao_previsores(df_Sneakers)\n",
        "Sneakers = Previsões(df_Sneakers, RNN(previsores, qt_real), escaler)"
      ],
      "metadata": {
        "id": "g8DV0X03dcAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Boot"
      ],
      "metadata": {
        "id": "f_5fl2v7eKz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sandals"
      ],
      "metadata": {
        "id": "ZCodjqpXi98_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Shoes"
      ],
      "metadata": {
        "id": "vbN7rOQqi-db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sneakers"
      ],
      "metadata": {
        "id": "YS3T226ai-sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizando o Prophet"
      ],
      "metadata": {
        "id": "FgOnvKiWTTxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prophet\n",
        "#Separando em Treino e Teste\n",
        "data_limite = pd.to_datetime(\"2023-02-01\").date()\n",
        "\n",
        "#set de treino\n",
        "train_ts = df_boot[:data_limite]\n",
        "train_ts = train_ts.drop('ds_product_line', axis = 1)\n",
        "#set de teste\n",
        "test_ts = df_boot[data_limite:]\n",
        "test_ts = test_ts.drop(columns='ds_product_line')\n",
        "\n",
        "print(\"Tamanho do conjunto de treino: \", train_ts.shape)\n",
        "print(\"Tamanho do conjunto de teste:  \", test_ts.shape)"
      ],
      "metadata": {
        "id": "ViU5Vk4DTdZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ajuste do dataset de treino\n",
        "train_ts = train_ts.reset_index()\n",
        "train_ts.columns = ['ds', 'y']\n",
        "train_ts.tail()"
      ],
      "metadata": {
        "id": "tvZBYbJTTl76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajuste do dataset de teste\n",
        "test_ts = test_ts.reset_index()\n",
        "test_ts.columns = ['ds', 'y']\n",
        "print(test_ts.head())\n",
        "print(test_ts.tail())"
      ],
      "metadata": {
        "id": "3MoAVNZuTnB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Prophet()\n",
        "m.fit(train_ts)"
      ],
      "metadata": {
        "id": "zy25boSJToh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construção do dataset de teste para alimentar o modelo\n",
        "#future = m.make_future_dataframe(periods=len(test_ts), freq = 'MS') # freq MS = períodos mensais\n",
        "future = m.make_future_dataframe(periods=1, freq = 'MS')\n",
        "# realiza previsão\n",
        "forecast = m.predict(future)"
      ],
      "metadata": {
        "id": "Xb0E4XzcTwwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot dos resultados\n",
        "m.plot(forecast);"
      ],
      "metadata": {
        "id": "rpB6jhjdTx8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot das componentes da série\n",
        "m.plot_components(forecast);"
      ],
      "metadata": {
        "id": "JFHZnWHfTzON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função para construir dataset com valores históricos e previsão realizada\n",
        "def make_comparison_dataframe(historical, forecast):\n",
        "    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))"
      ],
      "metadata": {
        "id": "cDKNZseMT0b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usa função para construir dataset\n",
        "prophet_forecast_1 = make_comparison_dataframe(pd.concat((train_ts, test_ts), axis = 0), forecast)\n",
        "\n",
        "prophet_forecast_1.tail()"
      ],
      "metadata": {
        "id": "U44LwBhAT1rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função reutilizável para cálculo de erros\n",
        "def calculate_forecast_errors(df, prediction_size):\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    df['e'] = df['y'] - df['yhat']\n",
        "    df['p'] = 100 * df['e'] / df['y']\n",
        "\n",
        "    predicted_part = df[-prediction_size:]\n",
        "\n",
        "    error_mean = lambda error_name: np.mean(np.abs(predicted_part[error_name]))\n",
        "\n",
        "    return {'MAPE': error_mean('p'), 'MAE': error_mean('e')}"
      ],
      "metadata": {
        "id": "nXfIhJaVT3Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cálculo de erros\n",
        "for err_name, err_value in calculate_forecast_errors(prophet_forecast_1, len(test_ts)).items():\n",
        "    print(err_name, err_value)\n",
        "    #Mean absolute percentage error\n",
        "    #Mean absolute error"
      ],
      "metadata": {
        "id": "HMUjejnsT4XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot dos resultados\n",
        "fig = plt.figure()\n",
        "plt.title(\"Resultados 1\")\n",
        "plt.plot(train_ts['ds'],train_ts['y'], color = 'black', label='Treino')\n",
        "plt.plot(test_ts['ds'],test_ts['y'],color='orange', label='Real')\n",
        "plt.plot(prophet_forecast_1['yhat'], color='red', label='Prophet')\n",
        "fig.autofmt_xdate()\n",
        "leg = plt.legend()"
      ],
      "metadata": {
        "id": "-WWNqQbqT5bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#yhat = previsto e y = valor real da base\n",
        "prophet_forecast_1.tail()"
      ],
      "metadata": {
        "id": "qeInmrVJT9f_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}